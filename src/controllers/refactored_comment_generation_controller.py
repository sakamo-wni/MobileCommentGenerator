"""ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼

å˜ä¸€è²¬ä»»åŸå‰‡ã«å¾“ã„ã€è²¬å‹™ã‚’é©åˆ‡ã«åˆ†é›¢ã€‚
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any
from collections.abc import Callable
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.types import LocationResult, BatchGenerationResult
from src.utils.error_handler import ErrorHandler
from src.utils.cpu_optimizer import CPUOptimizer
from src.workflows.comment_generation_workflow import run_comment_generation
from src.controllers.metadata_extractor import MetadataExtractor
from src.controllers.validators import ValidationManager
from src.controllers.batch_processor import BatchProcessor
from src.controllers.progress_handler import ProgressHandler
from src.controllers.async_batch_processor import AsyncBatchProcessor
from src.controllers.history_manager import HistoryManager
from src.controllers.config_manager import ConfigManager
from app_interfaces import ICommentGenerationController

logger = logging.getLogger(__name__)


class RefactoredCommentGenerationController(ICommentGenerationController):
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼
    
    å„è²¬å‹™ã‚’å°‚é–€ã®ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«å§”è­²ã—ã€è‡ªèº«ã¯ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ã¨ã—ã¦å‹•ä½œã€‚
    """
    
    def __init__(self, config=None):
        # å„ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
        self._config_manager = ConfigManager(config)
        self._history_manager = HistoryManager()
        self._metadata_extractor = MetadataExtractor()
        self._validators = ValidationManager(self._config_manager.config)
        self._progress_handler = ProgressHandler(self._metadata_extractor)
        self._batch_processor = BatchProcessor(self._progress_handler)
        
    # === è¨­å®šé–¢é€£ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆConfigManagerã«å§”è­²ï¼‰===
    
    @property
    def config(self):
        return self._config_manager.config
    
    def get_default_locations(self) -> list[str]:
        return self._config_manager.get_default_locations()
    
    def get_default_llm_provider(self) -> str:
        return self._config_manager.get_default_llm_provider()
    
    def get_config_dict(self) -> dict[str, str | int | float | bool]:
        return self._config_manager.get_config_dict()
    
    # === å±¥æ­´é–¢é€£ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆHistoryManagerã«å§”è­²ï¼‰===
    
    @property
    def generation_history(self) -> list[dict[str, str]]:
        return self._history_manager.generation_history
    
    # === æ¤œè¨¼é–¢é€£ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆValidationManagerã«å§”è­²ï¼‰===
    
    def validate_configuration(self) -> dict[str, bool | str]:
        return self._validators.validate_configuration()
    
    def validate_location_count(self, locations: list[str]) -> tuple[bool, str | None]:
        return self._validators.validate_location_count(locations)
    
    # === ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é–¢é€£ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆMetadataExtractorã«å§”è­²ï¼‰===
    
    def format_forecast_time(self, forecast_time: str) -> str:
        return self._metadata_extractor.format_forecast_time(forecast_time)
    
    def extract_weather_metadata(self, result: LocationResult) -> dict[str, str | float | None]:
        return self._metadata_extractor.extract_weather_metadata(result)
    
    # === ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ ===
    
    def generate_comment_for_location(self, location: str, llm_provider: str) -> LocationResult:
        """å˜ä¸€åœ°ç‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"""
        try:
            # å®Ÿéš›ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
            result = run_comment_generation(
                location_name=location,
                target_datetime=None,
                llm_provider=llm_provider
            )
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦åˆ¥ã‚¯ãƒ©ã‚¹ã«ç§»å‹•å¯èƒ½ï¼‰
            self._log_weather_timeline(location, result)
            
            # çµæœã‚’æ§‹ç¯‰
            location_result = self._build_location_result(location, result)
            
            # å±¥æ­´ã«ä¿å­˜
            self._history_manager.save_generation_result(result, location, llm_provider)
            
            return location_result
            
        except Exception as e:
            return ErrorHandler.create_error_result(location, e)
    
    def generate_comments_batch(
        self, 
        locations: list[str], 
        llm_provider: str,
        progress_callback: Callable[[int, int, str | None], None] | None = None, 
        max_workers: int | None = None
    ) -> BatchGenerationResult:
        """è¤‡æ•°åœ°ç‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰"""
        # æœ€é©ãªä¸¦åˆ—åº¦ã‚’æ±ºå®š
        if max_workers is None:
            max_workers = CPUOptimizer.get_io_bound_workers(
                task_count=len(locations),
                max_workers=16
            )
            logger.info(f"Optimized max_workers: {max_workers} (locations: {len(locations)})")
        
        # éåŒæœŸç‰ˆã®ä½¿ç”¨åˆ¤å®š
        if self._config_manager.is_async_weather_enabled():
            logger.info("ğŸš€ éåŒæœŸç‰ˆAPIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨")
            async_processor = AsyncBatchProcessor()
            try:
                return asyncio.run(
                    async_processor.generate_comments_batch_async(
                        locations, llm_provider, progress_callback
                    )
                )
            except Exception as e:
                logger.error(f"éåŒæœŸå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        
        # åŒæœŸç‰ˆã‚’ä½¿ç”¨
        logger.info("åŒæœŸç‰ˆAPIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨")
        return self._batch_processor.process_batch_sync(
            locations, llm_provider, 
            self.generate_comment_for_location,
            progress_callback, max_workers
        )
    
    def generate_with_progress(
        self, 
        locations: list[str], 
        llm_provider: str,
        view, 
        results_container
    ) -> BatchGenerationResult:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆï¼ˆStreamlit UIé€£æºï¼‰"""
        import streamlit as st
        from app_session_manager import SessionManager
        
        # UIåˆæœŸåŒ–
        with results_container.container():
            st.markdown("### ğŸŒ¤ï¸ ç”Ÿæˆçµæœ")
        
        progress_bar, status_text = view.create_progress_ui()
        SessionManager.set_generating(True)
        
        all_results = []
        
        try:
            # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
            def progress_callback(idx: int, total: int, location: str) -> None:
                self._progress_handler.update_progress(
                    progress_bar, status_text, idx, total, location,
                    results_container, all_results, view
                )
            
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            return self._execute_batch_with_progress(
                locations, llm_provider, progress_callback, 
                all_results, results_container, view
            )
            
        finally:
            SessionManager.set_generating(False)
            progress_bar.empty()
            status_text.empty()
    
    # === ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ ===
    
    def _log_weather_timeline(self, location: str, result: dict[str, Any]) -> None:
        """weather_timelineã®ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°å‡ºåŠ›"""
        generation_metadata = result.get('generation_metadata', {})
        weather_timeline = generation_metadata.get('weather_timeline', {})
        if weather_timeline:
            future_forecasts = weather_timeline.get('future_forecasts', [])
            logger.info(f"Controller: location={location}, weather_timelineå­˜åœ¨, future_forecastsæ•°={len(future_forecasts)}")
            if future_forecasts:
                logger.debug(f"Controller: future_forecasts[0]={future_forecasts[0]}")
        else:
            logger.warning(f"Controller: location={location}, weather_timelineãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    
    def _build_location_result(self, location: str, result: dict[str, Any]) -> LocationResult:
        """LocationResultå‹ã®çµæœã‚’æ§‹ç¯‰"""
        location_result: LocationResult = {
            'location': location,
            'result': result,
            'success': result.get('success', False),
            'comment': result.get('final_comment', ''),
            'error': result.get('error', None),
            'source_files': None
        }
        
        # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’æŠ½å‡º
        metadata = result.get('generation_metadata', {})
        if metadata.get('selected_past_comments'):
            sources = []
            for comment in metadata['selected_past_comments']:
                if 'source_file' in comment:
                    sources.append(comment['source_file'])
            if sources:
                location_result['source_files'] = sources
                # è©³ç´°ãƒ­ã‚°å‡ºåŠ›
                logger.info(f"åœ°ç‚¹: {location}")
                logger.info(f"  å¤©æ°—: {metadata.get('weather_condition', 'ä¸æ˜')}")
                logger.info(f"  æ°—æ¸©: {metadata.get('temperature', 'ä¸æ˜')}Â°C")
                logger.info(f"  ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆå…ƒãƒ•ã‚¡ã‚¤ãƒ«: {sources}")
                logger.info(f"  ç”Ÿæˆã‚³ãƒ¡ãƒ³ãƒˆ: {result.get('final_comment', '')}")
        
        return location_result
    
    def _execute_batch_with_progress(
        self,
        locations: list[str],
        llm_provider: str,
        progress_callback: Callable,
        all_results: list[LocationResult],
        results_container,
        view
    ) -> BatchGenerationResult:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºä»˜ããƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œ"""
        total_locations = len(locations)
        
        # æœ€é©ãªä¸¦åˆ—åº¦ã‚’æ±ºå®š
        max_workers = CPUOptimizer.get_io_bound_workers(
            task_count=total_locations,
            max_workers=16
        )
        logger.info(f"Optimized max_workers for batch: {max_workers}")
        
        # ä¸¦åˆ—å‡¦ç†ã§è¤‡æ•°åœ°ç‚¹ã‚’å‡¦ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # å„åœ°ç‚¹ã®å‡¦ç†ã‚’ã‚µãƒ–ãƒŸãƒƒãƒˆ
            future_to_location = {}
            for location in locations:
                future = executor.submit(
                    self.generate_comment_for_location,
                    location,
                    llm_provider
                )
                future_to_location[future] = location
            
            # å®Œäº†ã—ãŸé †ã«çµæœã‚’å‡¦ç†
            completed_count = 0
            for future in as_completed(future_to_location):
                location = future_to_location[future]
                completed_count += 1
                
                # é€²æ—æ›´æ–°
                progress_callback(completed_count - 1, total_locations, location)
                
                # çµæœã‚’å–å¾—
                try:
                    location_result = future.result()
                    all_results.append(location_result)
                except Exception as e:
                    logger.error(f"Error processing {location}: {e}")
                    location_result = ErrorHandler.create_error_result(location, e)
                    all_results.append(location_result)
        
        # çµæœã‚’é›†ç´„
        return self._progress_handler.aggregate_results(all_results, locations)