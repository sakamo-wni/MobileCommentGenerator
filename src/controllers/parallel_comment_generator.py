"""
Parallel comment generation processor

ä¸¦åˆ—ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
"""

from __future__ import annotations
import asyncio
import logging
from typing import Any, Optional, Dict, List
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from src.workflows.comment_generation_workflow import run_comment_generation
from src.types import LocationResult, BatchGenerationResult
from src.utils.error_handler import ErrorHandler

logger = logging.getLogger(__name__)


class ParallelCommentGenerator:
    """ä¸¦åˆ—ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆå™¨
    
    è¤‡æ•°åœ°ç‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¦
    ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ã‚’çŸ­ç¸®
    """
    
    def __init__(self, 
                 max_workers: int = 4,
                 timeout_per_location: int = 30):
        """åˆæœŸåŒ–
        
        Args:
            max_workers: æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            timeout_per_location: åœ°ç‚¹ã”ã¨ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
        """
        self.max_workers = max_workers
        self.timeout_per_location = timeout_per_location
        self._lock = threading.Lock()
        self._stats = {
            "parallel_processed": 0,
            "serial_processed": 0,
            "timeout_count": 0,
            "error_count": 0
        }
    
    def generate_parallel(self,
                         locations_with_weather: Dict[str, Any],
                         llm_provider: str = "gemini",
                         progress_callback: Optional[callable] = None) -> BatchGenerationResult:
        """è¤‡æ•°åœ°ç‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä¸¦åˆ—ç”Ÿæˆ
        
        Args:
            locations_with_weather: åœ°ç‚¹åã¨å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
            llm_provider: LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
        Returns:
            ãƒãƒƒãƒç”Ÿæˆçµæœ
        """
        start_time = datetime.now()
        all_results = []
        completed_count = 0
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ±ºå®šï¼ˆå¤§é‡ã®åœ°ç‚¹ã®å ´åˆã¯åˆ¶é™ï¼‰
        use_parallel = len(locations_with_weather) > 1 and len(locations_with_weather) <= 20
        
        if use_parallel:
            logger.info(f"ğŸš€ {len(locations_with_weather)}åœ°ç‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä¸¦åˆ—ç”Ÿæˆé–‹å§‹ï¼ˆæœ€å¤§{self.max_workers}ä¸¦åˆ—ï¼‰")
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # å„åœ°ç‚¹ã®ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
                future_to_location = {}
                
                for location, weather_data in locations_with_weather.items():
                    if weather_data:
                        future = executor.submit(
                            self._generate_single_comment,
                            location,
                            weather_data,
                            llm_provider
                        )
                        future_to_location[future] = location
                    else:
                        # å¤©æ°—ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼çµæœã‚’è¿½åŠ 
                        result = ErrorHandler.create_error_result(
                            location,
                            ValueError("å¤©æ°—äºˆå ±ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                        )
                        all_results.append(result)
                        completed_count += 1
                        
                        if progress_callback:
                            progress_callback(completed_count, len(locations_with_weather), location)
                
                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰é †ã«å‡¦ç†
                for future in as_completed(future_to_location, timeout=self.timeout_per_location * len(future_to_location)):
                    location = future_to_location[future]
                    
                    try:
                        result = future.result(timeout=self.timeout_per_location)
                        all_results.append(result)
                        
                        with self._lock:
                            self._stats["parallel_processed"] += 1
                            
                    except TimeoutError:
                        logger.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {location}")
                        result = ErrorHandler.create_error_result(
                            location,
                            TimeoutError(f"ã‚³ãƒ¡ãƒ³ãƒˆç”ŸæˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{self.timeout_per_location}ç§’ï¼‰")
                        )
                        all_results.append(result)
                        
                        with self._lock:
                            self._stats["timeout_count"] += 1
                            
                    except Exception as e:
                        logger.error(f"ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {location} - {e}")
                        result = ErrorHandler.create_error_result(location, e)
                        all_results.append(result)
                        
                        with self._lock:
                            self._stats["error_count"] += 1
                    
                    completed_count += 1
                    if progress_callback:
                        progress_callback(completed_count, len(locations_with_weather), location)
        
        else:
            # ã‚·ãƒªã‚¢ãƒ«å‡¦ç†ï¼ˆå°‘æ•°ã¾ãŸã¯å¤§é‡ã®å ´åˆï¼‰
            logger.info(f"ğŸ“ {len(locations_with_weather)}åœ°ç‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã‚·ãƒªã‚¢ãƒ«ç”Ÿæˆ")
            
            for location, weather_data in locations_with_weather.items():
                try:
                    if weather_data:
                        result = self._generate_single_comment(location, weather_data, llm_provider)
                    else:
                        result = ErrorHandler.create_error_result(
                            location,
                            ValueError("å¤©æ°—äºˆå ±ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                        )
                    
                    all_results.append(result)
                    
                    with self._lock:
                        self._stats["serial_processed"] += 1
                        
                except Exception as e:
                    logger.error(f"ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {location} - {e}")
                    result = ErrorHandler.create_error_result(location, e)
                    all_results.append(result)
                    
                    with self._lock:
                        self._stats["error_count"] += 1
                
                completed_count += 1
                if progress_callback:
                    progress_callback(completed_count, len(locations_with_weather), location)
        
        # çµæœã‚’é›†è¨ˆ
        success_count = sum(1 for r in all_results if r["success"])
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(
            f"âœ… ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆå®Œäº†: "
            f"æˆåŠŸ={success_count}/{len(locations_with_weather)}, "
            f"æ™‚é–“={processing_time:.1f}ç§’, "
            f"ä¸¦åˆ—={self._stats['parallel_processed']}, "
            f"ã‚·ãƒªã‚¢ãƒ«={self._stats['serial_processed']}"
        )
        
        return BatchGenerationResult(
            results=all_results,
            total_count=len(locations_with_weather),
            success_count=success_count,
            failed_count=len(locations_with_weather) - success_count,
            processing_time=processing_time
        )
    
    def _generate_single_comment(self,
                               location: str,
                               weather_data: Any,
                               llm_provider: str) -> LocationResult:
        """å˜ä¸€åœ°ç‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            location: åœ°ç‚¹å
            weather_data: å¤©æ°—ãƒ‡ãƒ¼ã‚¿
            llm_provider: LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
            
        Returns:
            åœ°ç‚¹çµæœ
        """
        try:
            # ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
            result = run_comment_generation(
                location_name=location,
                llm_provider=llm_provider,
                pre_fetched_weather=weather_data
            )
            
            return LocationResult(
                location=location,
                success=True,
                comment=result.get("final_comment", ""),
                advice=result.get("advice_comment", ""),
                weather_summary=result.get("weather_summary", ""),
                generation_metadata=result.get("generation_metadata", {})
            )
            
        except Exception as e:
            logger.error(f"ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {location} - {e}")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        with self._lock:
            total_processed = (
                self._stats["parallel_processed"] + 
                self._stats["serial_processed"]
            )
            
            return {
                "total_processed": total_processed,
                "parallel_processed": self._stats["parallel_processed"],
                "serial_processed": self._stats["serial_processed"],
                "timeout_count": self._stats["timeout_count"],
                "error_count": self._stats["error_count"],
                "max_workers": self.max_workers,
                "timeout_per_location": self.timeout_per_location
            }
    
    async def generate_parallel_async(self,
                                    locations_with_weather: Dict[str, Any],
                                    llm_provider: str = "gemini",
                                    progress_callback: Optional[callable] = None) -> BatchGenerationResult:
        """éåŒæœŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆå†…éƒ¨ã§ã¯åŒæœŸå‡¦ç†ï¼‰
        
        asyncioã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—å†…ã‹ã‚‰å‘¼ã³å‡ºã™ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼
        """
        loop = asyncio.get_event_loop()
        
        # ThreadPoolExecutorã§å®Ÿè¡Œ
        with ThreadPoolExecutor(max_workers=1) as executor:
            result = await loop.run_in_executor(
                executor,
                self.generate_parallel,
                locations_with_weather,
                llm_provider,
                progress_callback
            )
        
        return result